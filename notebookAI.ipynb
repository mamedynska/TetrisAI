{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Exemple d'avant :\n",
    "\n",
    "# # Premier réseau de neuronnes : reconnaissances d'un chiffre écrit à la main :\n",
    "\n",
    "# # Les deux jeux de données sont des jeu de données connues et utilisé pour apprendre les NN, on les téléchargent directement grâce au download=True, \n",
    "# #  On les téléchargent dans le répertoire du premier paramètre de MNIST ici : ''\n",
    "\n",
    "# # Jeu de données pour l'apprentissage\n",
    "# train = datasets.MNIST('', train=True, download=True,\n",
    "#                        transform=transforms.Compose([\n",
    "#                            transforms.ToTensor()\n",
    "#                        ]))\n",
    "# # Jeu de données pour les tests\n",
    "# test = datasets.MNIST('', train=False, download=True,\n",
    "#                        transform=transforms.Compose([\n",
    "#                            transforms.ToTensor()\n",
    "#                        ]))\n",
    "\n",
    "# # On mélange les données et batch_size c'est combien de données à la fois on passe mais comme on va traiter des images de 28*28, nos CPU pouuraient tous traiter d'un coup\n",
    "# trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "# testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)\n",
    "\n",
    "# # La classe qui représente le réseau de neuronnes, elle hérites de nn.Module\n",
    "# class Net(nn.Module):\n",
    "\n",
    "# # On utilise init de nn.Module avec super()\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         input = 28*28\n",
    "#         output = output2 = output3 = 64\n",
    "#         lastOutput = 10         # On veut reconnaître des chiffres donc la dernière sortie sera entre [0,9] donc de tailles 10\n",
    "\n",
    "#         # On créer nos couches de neuronnes\n",
    "#         self.fc1 = nn.Linear(input, output)         # Nb input = le nombre de valeur à prendre en compte pour la décision \n",
    "#                                                     # Nb output le nombre de choix possible pour une décision\n",
    "#         self.fc2 = nn.Linear(output, output2)       # 1Deuxième couche de neuronnes\n",
    "#         self.fc3 = nn.Linear(output2, output3)      # Troisième : Possibilité que output=output2=output3\n",
    "#         self.fc4 = nn.Linear(output3, lastOutput)\n",
    "\n",
    "\n",
    "# # Pour chaque données on l'a fait passer à travers notre réseau de neuronnes\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))         # F.relu transforme notre x (qui peut être = 15605 par ex) dans un intervalle [0,1]\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         x = self.fc4(x)                 # Pour notre dernière couche qui sera une matrice de taille [1, nb de coup jouable] \n",
    "\n",
    "#         return F.log_softmax(x, dim=1)  #On aura une matrice de taille 1,10 pour la reconnaissance des chiffres avec une probabilité que ce soit chacun des chiffres\n",
    "\n",
    "# # Ici c'est mon main hihi\n",
    "# net = Net()\n",
    "# # print(net)\n",
    "\n",
    "# # x = torch.rand((1,10))\n",
    "# # print(x)\n",
    "# # x = x.view(1,10)\n",
    "# # output=net(x)\n",
    "# # print(output)\n",
    "\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001)      # lr = learning rate : permet de dire à l'optimizer de ne pas sur apprendre sur chaque données\n",
    "#                                                         # On ne va pas que lorsqu'il se trompe dans sa prédiction, il se corrige pour avoir une probabilité de 0 partout et de 1 sur le bon chiffre\n",
    "#                                                         # Sinon on va sur-apprendre et ce n'est pas bon\n",
    "#                                                         # On va donc optimiser la perte \"Loss\" sans la rendre nulle pour éviter le sur-apprentissage\n",
    "# EPOCHS = 3\n",
    "# for epoch in range(EPOCHS): # 3 full passes over the data\n",
    "#     for data in trainset:  # `data` is a batch of data\n",
    "#          X, y = data  # X sont les données d'une image 28*28, y est le label càd le chiffre que représente l'image\n",
    "#          net.zero_grad()    # On reset pour chaque data\n",
    "#          output = net(X.view(-1,28*28))  # On fait passer notre data dans le NN (-1 pour faire ligne --> colonne)\n",
    "#          loss = F.nll_loss(output, y)   # On calcul la perte avec l'output qu'on a eu et y le résultat voulu\n",
    "#          loss.backward()                # Calcul tous seul le gradient (merci Pytorch)     \n",
    "#          optimizer.step()               # Lance une étape d'optimisatin\n",
    "#     print(loss)\n",
    "\n",
    "# # On va voir à quel point on est correcte : \n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# with torch.no_grad():       # On ne veut pas des gradient ici on va juste regarder si on a bon ou pas sur la valeur avec le plus de proba\n",
    "#     for data in testset:\n",
    "#         X, y = data\n",
    "#         output = net(X.view(-1,784))\n",
    "#         #print(output)\n",
    "#         for idx, i in enumerate(output):\n",
    "#             #print(torch.argmax(i), y[idx])\n",
    "#             if torch.argmax(i) == y[idx]:\n",
    "#                 correct += 1\n",
    "#             total += 1\n",
    "\n",
    "# print(\"Accuracy: \", round(correct/total, 3))    # On arrondi la précision\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Afficher la première image\n",
    "# plt.imshow(X[0].view(28,28))\n",
    "# plt.show()\n",
    "\n",
    "# # Afficher notre prédiction (avec la proba la + élévé) de notre modèle\n",
    "# print(torch.argmax(net(X[0].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Linear(4, 64), nn.ReLU(inplace=True))     # Sequential : Linear and ReLu\n",
    "        self.conv2 = nn.Sequential(nn.Linear(64, 64), nn.ReLU(inplace=True))    # Sequential : Linear and ReLu\n",
    "        self.conv3 = nn.Sequential(nn.Linear(64, 1))\n",
    "        self._create_weights()\n",
    "\n",
    "    def _create_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)             # Loi uniforme\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # Au lieu de faire le RELU ici on le fait avant dans __init__\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        return x\n",
    "print(\"Done\")"
   ]
  },
  {
   "source": [
    "# Fichier qui va entrainer notre réseau de neuronnes\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import tetris\n",
    "\n",
    "def get_args():\n",
    "    # Paramètre de la grid Tetris\n",
    "    parser = argparse.ArgumentParser(\n",
    "        \"\"\"Implementation of Deep Q Network to play Tetris\"\"\")\n",
    "    parser.add_argument(\"--width\", type=int, default=10, help=\"The common width for all images\")\n",
    "    parser.add_argument(\"--height\", type=int, default=15, help=\"The common height for all images\")\n",
    "    # parser.add_argument(\"--block_size\", type=int, default=30, help=\"Size of a block\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=512, help=\"The number of images per batch\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3)                   # Learning rate  : permet de dire à l'optimizer de ne pas sur apprendre sur chaque données\n",
    "                                                                            # On ne va pas que lorsqu'il se trompe dans sa prédiction, il se corrige pour avoir une probabilité de 0 \n",
    "                                                                            # partout et de 1 sur le bon chiffre\n",
    "                                                                            # Sinon on va sur-apprendre et ce n'est pas bon.\n",
    "                                                                            # On va donc optimiser la perte \"Loss\" sans la rendre nulle pour éviter le sur-apprentissage\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--initial_epsilon\", type=float, default=1)\n",
    "    parser.add_argument(\"--final_epsilon\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--num_decay_epochs\", type=float, default=2000)\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=3000)             # Nombre de génération\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=1000)          # Intervalle de sauvegarde du réseau\n",
    "    parser.add_argument(\"--replay_memory_size\", type=int, default=30000,    \n",
    "                        help=\"Number of epoches between testing phases\")\n",
    "    parser.add_argument(\"--log_path\", type=str, default=\"tensorboard\")      # Chemin des logs\n",
    "    parser.add_argument(\"--saved_path\", type=str, default=\"trained_models\") # Chemin de sauvegarde\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "# Fonction d'apprentissage\n",
    "def train():\n",
    "    # Cuda permet d'éxécuter sur le GPU, si on peut le faire on l'utilise\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(123)\n",
    "    # Sinon ça sera sur le CPU\n",
    "    else:\n",
    "        torch.manual_seed(123)\n",
    "    # Récupération d'un log\n",
    "    if os.path.isdir(opt.log_path):\n",
    "        shutil.rmtree(opt.log_path)\n",
    "    os.makedirs(opt.log_path)\n",
    "    writer = SummaryWriter(opt.log_path)\n",
    "    env = Tetris()          # Notre jeu tetris, ici c'est l'environnement de l'IA d'où \"env\"\n",
    "    net = DeepQNetwork()    # Notre réseau de neuronnes\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)     # Notre optimizer\n",
    "    criterion = nn.MSELoss()        # Fonction de perte MeanSquar : va pénaliser le model pour de trop grande erreurs et encourage le model quand il en fait de petite\n",
    "\n",
    "    state = env.reset()         # Remettre à zéro le tetris et retourner l'état \n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "        state = state.cuda()\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}